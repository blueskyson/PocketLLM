version: '3.8'

services:
  # Model Downloader
  model-downloader:
    image: alpine:latest
    container_name: model-downloader
    volumes:
      - ./models:/models
    command: >
      sh -c "
        if [ ! -f /models/Phi-3-mini-4k-instruct-q4.gguf ]; then
          apk add --no-cache wget &&
          wget -O /models/Phi-3-mini-4k-instruct-q4.gguf https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
        else
          echo 'Model already exists, skipping download'
        fi
      "

  # LLM Model Server
  llm:
    image: ghcr.io/ggerganov/llama.cpp:server-b4667
    platform: linux/amd64
    container_name: pocketllm-llm
    ports:
      - "8081:8080"
    volumes:
      - ./models:/models:ro
    command: >
      -m /models/Phi-3-mini-4k-instruct-q4.gguf
      --host 0.0.0.0
      --port 8080
      -c 4096
      -ngl 0
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G
    restart: unless-stopped

  # Spring Boot Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: pocketllm-backend
    ports:
      - "8080:8080"
    environment:
      - LLM_CLIENT_URL=http://llm:8080/v1/chat/completions
      - LLM_CLIENT_MODEL=Phi-3-mini-4k-instruct-q4
      - LLM_CLIENT_TIMEOUT=120s
    depends_on:
      - llm
    restart: unless-stopped

  # React Frontend
  frontend:
    build:
      context: ./create-anything/apps/web
      dockerfile: Dockerfile
    container_name: pocketllm-frontend
    ports:
      - "4000:4000"
    environment:
      - VITE_API_URL=http://backend:8080
    depends_on:
      - backend
    restart: unless-stopped

networks:
  default:
    name: pocketllm-network
